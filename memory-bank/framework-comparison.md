# Comparative Analysis: RAGLab vs. Professional Evaluation Frameworks

**Date:** January 3, 2026
**Purpose:** Honest comparison of RAGLab against academic/industry RAG evaluation benchmarks

## Executive Summary

RAGLab is a **well-structured learning project** that implements real research concepts but operates at a fundamentally different scale and scope than the professional frameworks. This isn't a criticismâ€”it's the expected difference between a personal learning tool and academic/industry benchmarks.

| Aspect | RAGBench | SCARF | BenchmarkQED | **RAGLab** |
|--------|----------|-------|--------------|------------|
| **Scale** | 100k examples | Multi-framework | 1,400+ articles | 19 books |
| **Domains** | 5 industry | Deployed systems | News/Podcasts | 2 (neuro + phil) |
| **Focus** | Metrics research | Black-box testing | GraphRAG comparison | Learning/exploration |
| **Questions** | ~10k per domain | Dataset-based | Synthetic (AutoQ) | 15-45 handcrafted |
| **Maturity** | Academic paper | Industry tool | Microsoft Research | Personal project |

---

## 1. Document Processing & Chunking

### RAGBench
- **No unified chunking**: Uses pre-chunked datasets. PubMedQA uses "already segmented abstracts." CUAD uses entire contracts (up to 11k tokens). Other datasets embed passages with `text-embedding-ada-002` and retrieve 3-10 documents.
- **Rationale**: Focused on evaluation metrics, not chunking comparison.

### SCARF
- **Minimal detail**: Framework agnosticâ€”evaluates existing RAG systems as black boxes. Chunking is delegated to the frameworks being tested (Giskard, Cheshire Cat, etc.).

### BenchmarkQED
- **Standardized for comparison**: Tests LazyGraphRAG with chunk sizes of **200 or 600 tokens**. All methods standardized to 8,000 tokens for answer generation.
- **Focus**: Compares GraphRAG variants, not chunking strategies.

### RAGLab
- **Docling â†’ Markdown â†’ spaCy segmentation â†’ 800-token chunks with 2-sentence overlap**
- **Multiple strategies**: section, contextual (LLM-prepended context), raptor (hierarchical GMM + summarization), semantic (sentence boundary detection)
- **Assessment**: RAGLab's chunking pipeline is **more sophisticated than most benchmarks**. RAGBench doesn't even standardize chunking. The approach of testing multiple chunking strategies is closer to RAPTOR research than evaluation frameworks.

**Verdict**: âœ… RAGLab's chunking approach is **above average** for evaluation frameworks. Most focus on metrics, not document processing.

---

## 2. Test Question Generation

### RAGBench
- **Mixed sources**: Crowd-sourced (HotpotQA, EManual), expert-composed (CovidQA, CUAD), real user queries (MS Marco, TechQA), automated (PubMedQA).
- **Responses generated by**: GPT-3.5 and Claude 3 Haiku with temperature 1.0.
- **Ground truth labels**: GPT-4 annotates span-level relevance, utilization, adherence via chain-of-thought prompting with JSON schema.

### SCARF
- **Uses existing benchmarks**: LangChain Docs QA and other retrieval benchmarks. No custom generation methodology.

### BenchmarkQED
- **AutoQ (fully synthetic)**: 2Ã—2 design space:
  - Data-local: Questions from specific passages
  - Data-global: Questions requiring corpus-wide reasoning
  - Activity-local: Task-driven narrow scope
  - Activity-global: Task-driven comprehensive analysis
- **Process**: Generates dataset summaries â†’ personas â†’ tasks â†’ candidate queries â†’ clustering â†’ final selection.
- **No reference answers**: Generates assertions for validation but no gold answers.

### RAGLab
- **Handcrafted with Claude Opus 4.5**: 15-45 questions based on deep reading of 19 books.
- **Ground truth**: Specific quotes from expected books (e.g., "the amygdala expands in size with long-term PTSD").
- **Cross-domain questions**: 10 questions explicitly require retrieval from multiple books across domains.
- **Difficulty taxonomy**: single_concept vs. cross_domain.

**Assessment**: RAGLab's approach is **qualitatively different** from all three frameworks:
- **Strength**: Questions have genuine ground truth with exact quotes, enabling precise context_recall evaluation. RAGBench uses GPT-4 for labeling. BenchmarkQED has no reference answers.
- **Weakness**: 15 questions is orders of magnitude smaller than 100k (RAGBench) or synthetic generation (AutoQ).
- **Unique**: Cross-domain questions (requiring neuroscience + philosophy synthesis) test a capability most benchmarks ignore.

**Verdict**: ðŸŸ¡ **High quality, low quantity**. Questions are arguably higher quality than GPT-generated labels, but statistical significance is limited.

---

## 3. Datasets & Domains

### RAGBench
- **12 sub-datasets across 5 domains**:
  - Biomedical: PubMedQA, CovidQA
  - General: HotpotQA, MS Marco, HAGRID, ExpertQA
  - Legal: CUAD
  - Customer support: DelucionQA, EManual, TechQA
  - Finance: FinQA, TAT-QA
- **Industry-relevant**: User manuals, contracts, financial reports.

### SCARF
- **LangChain Docs QA**: Technical documentation domain.
- **Flexible**: Supports adding custom datasets.

### BenchmarkQED
- **AP News**: 1,397 health-related articles.
- **Behind the Tech podcasts**: 70 episodes.
- **Scope**: News/conversation analysis, not document QA.

### RAGLab
- **19 books** in 2 domains:
  - Neuroscience: Sapolsky, Eagleman, Gazzaniga, Kahneman, etc.
  - Philosophy: Stoics (Epictetus, Marcus Aurelius, Seneca), Taoism, Schopenhauer
- **Total estimated**: ~500k-1M tokens of source content.

**Assessment**:
- RAGLab corpus is **deeply thematic** vs. broadly diverse. This is a design choice, not a flaw.
- Testing cross-domain reasoning (neuroscience â†” philosophy) is a **unique capability** no benchmark explicitly tests.
- Evaluating **long-form book synthesis** is harder than short-passage retrieval.

**Verdict**: ðŸŸ¢ **Niche but intentional**. Not comparable in breadth but tests harder synthesis tasks.

---

## 3.1 RAGBench Deep Dive: Architecture & Dataset Differences

RAGBench deserves special attention as the largest academic RAG benchmark. This section details the fundamental architectural differences.

### Architectural Difference: Pre-Retrieved vs End-to-End

```
RAGBench Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question   â”‚ --> â”‚  Documents  â”‚ --> â”‚  Response   â”‚
â”‚             â”‚     â”‚ (PROVIDED)  â”‚     â”‚ (generated) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    Already retrieved - no retrieval step tested

RAGLab Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question   â”‚ --> â”‚  Retrieval  â”‚ --> â”‚  Documents  â”‚ --> â”‚  Response   â”‚
â”‚             â”‚     â”‚  (Weaviate) â”‚     â”‚ (retrieved) â”‚     â”‚ (generated) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
              Tests chunking + embedding + search strategies
```

**Key implication**: RAGBench evaluates **generation quality** given perfect retrieval. RAGLab evaluates the **full pipeline** including retrieval quality.

### RAGBench 12 Sub-Datasets

| Dataset | Domain | Docs/Example | Avg Tokens | Train/Val/Test | Source Type |
|---------|--------|--------------|------------|----------------|-------------|
| PubMedQA | Biomedical | 4 | 99 | 19.5k/2.5k/2.5k | Research abstracts |
| CovidQA | Biomedical | 4 | 122 | 2.5k/534/492 | CORD-19 papers |
| HotpotQA | General | 4 | 126 | 3.7k/847/776 | Wikipedia paragraphs |
| MS Marco | General | 10 | 94 | ~8k/1k/1k | Bing search results |
| HAGRID | General | 3 | 153 | ~1.5k/250/250 | Wikipedia passages |
| ExpertQA | General | 1 | 548 | ~1.5k/250/250 | Google search results |
| CUAD | Legal | 1 | 11,000 | 1.5k/506/508 | Commercial contracts |
| FinQA | Finance | 3 | 310 | 12k/1.7k/2.2k | Financial reports |
| TAT-QA | Finance | 5 | 96 | 26k/3.2k/3.2k | Tables + text |
| TechQA | Support | 10 | 1,800 | ~2k/300/300 | IBM forum posts |
| EManual | Support | 3 | ~200 | ~1.5k/250/250 | Samsung TV manual |
| DelucionQA | Support | 3 | ~150 | ~1.5k/250/250 | Jeep manual |

**Total**: ~100k examples (78k train, 12k validation, 11k test)

### RAGLab Corpus

| Domain | Books | Est. Tokens | Questions | Source Type |
|--------|-------|-------------|-----------|-------------|
| Neuroscience | ~10 | ~400k | 8 | Academic/popular science books |
| Philosophy | ~9 | ~300k | 7 | Classical texts + modern interpretations |
| **Cross-domain** | 19 | ~700k | 10 | Multi-book synthesis required |

**Total**: 19 books, ~700k tokens, 15-45 questions

### Ground Truth Comparison

**RAGBench** (no human reference answers):
```python
{
    "question": "What was the revenue change?",
    "documents": ["Table: Revenue $500M (2019), $550M (2020)..."],
    "response": "Revenue increased by 10%.",  # LLM-generated answer

    # TRACe labels (GPT-4 annotated grounding, not correctness)
    "relevance_score": 0.85,      # Are docs relevant to question?
    "utilization_score": 0.72,    # How much context was used?
    "completeness_score": 0.90,   # Was all relevant info used?
    "adherence_score": True,      # Is response grounded (no hallucination)?
}
```

**RAGLab** (human-crafted reference answers):
```python
{
    "question": "Why do humans struggle with self-control?",
    "reference": "Self-control failures stem from brain architecture...",  # Gold answer
    "ground_truth_quote": "\"System 1 is more influential than your experience\" (Kahneman)",
    "expected_books": ["Thinking Fast and Slow", "Behave", "The Enchiridion"],
}
```

| Aspect | RAGBench | RAGLab |
|--------|----------|--------|
| Reference answers | âŒ None (response IS the data) | âœ… Human-crafted with quotes |
| Grounding labels | âœ… Span-level TRACe | âŒ Not annotated |
| Source attribution | âŒ Not tracked | âœ… Expected books listed |
| Enables context_recall | âŒ No reference | âœ… Yes |
| Enables answer_correctness | âŒ No reference | âœ… Yes |
| Enables utilization metrics | âœ… TRACe labels | âŒ Not implemented |

### Scope Comparison

| Dimension | RAGBench | RAGLab |
|-----------|----------|--------|
| **What it tests** | Generation quality given docs | Full pipeline (retrieval + generation) |
| **Retrieval tested** | âŒ Documents pre-attached | âœ… Weaviate hybrid search |
| **Chunking tested** | âŒ Pre-chunked passages | âœ… 4 chunking strategies |
| **Question types** | Single-hop, multi-hop, numerical | Single-concept, cross-domain synthesis |
| **Document length** | 94-11,000 tokens per doc | 800-token chunks from books |
| **Domain breadth** | 5 industries, 12 datasets | 2 domains, 1 corpus |
| **Statistical power** | 100k examples | 15-45 examples |
| **Reproducibility** | Public HuggingFace dataset | Private corpus |

### Key Insight

RAGBench and RAGLab answer **different questions**:

- **RAGBench asks**: "Given these documents, can your LLM generate a grounded answer?"
- **RAGLab asks**: "Given these books, can your full RAG pipeline find relevant chunks AND generate a correct answer?"

RAGBench is a **generation benchmark**. RAGLab is an **end-to-end RAG exploration platform**.

---

## 4. Hyperparameter Handling

### RAGBench
- **None**: Fixed retrieval (FAISS with Euclidean distance, 4-10 chunks). Focus is on evaluation metrics, not RAG configuration.

### SCARF
- **Framework-level**: Tests different RAG frameworks (Qdrant vs. Milvus) and LLM backends (OpenLLM vs. Ollama). No fine-grained hyperparameter grids.

### BenchmarkQED
- **Limited**: LazyGraphRAG configurations:
  - Chunk sizes: 200, 600 tokens
  - Query budgets: 50, 200 subqueries
  - Models: GPT-4o vs. GPT-4o mini
- **Focus**: Comparing methods, not tuning hyperparameters.

### RAGLab
- **5D Grid Search**:
  ```
  collections Ã— search_types Ã— alphas Ã— strategies Ã— top_k
      4-5     Ã—      2       Ã—  2-3   Ã—     4      Ã— 2 = ~100-150 combinations
  ```
  - Collections: section, contextual, raptor, semantic variants
  - Search types: keyword (BM25), hybrid (vector + BM25)
  - Alpha: 0.5, 1.0 (for hybrid)
  - Strategies: none, hyde, decomposition, graphrag
  - Top-K: 10, 20

**Assessment**: RAGLab's hyperparameter grid is **more thorough than any of the three frameworks**. RAGBench doesn't vary hyperparameters. BenchmarkQED tests 4-8 configurations. RAGLab tests 100+.

**Verdict**: âœ…âœ… **Best-in-class hyperparameter exploration**. This is genuinely thorough.

---

## 5. System Focus & Generality

### RAGBench
- **General-purpose**: Designed for any RAG system. Provides 100k labeled examples for training custom evaluators.
- **Actionable**: TRACe metrics designed to identify specific failure modes (low utilization, hallucination, incomplete responses).

### SCARF
- **Deployed RAG systems**: Black-box evaluation of production applications. Tests across vector databases and LLM backends.
- **Practical**: Meant for comparing commercial RAG solutions.

### BenchmarkQED
- **GraphRAG-focused**: Explicitly designed to evaluate LazyGraphRAG vs. alternatives (RAPTOR, LightRAG, vector RAG).
- **Local vs. Global queries**: Core insight that global queries benefit from graph approaches.

### RAGLab
- **Learning-focused**: Designed to understand RAG concepts through implementation.
- **Broad open questions**: Tests synthesis across two knowledge domains (neuroscience + philosophy).
- **Modular exploration**: Each strategy (HyDE, decomposition, graphrag, RAPTOR) implemented from research papers.

**Assessment**: RAGLab is **orthogonal** to these benchmarks:
- They're for **evaluating** existing systems.
- RAGLab is for **understanding** how RAG works by building it.
- Focus on cross-domain synthesis (philosophy + neuroscience) is unique.

**Verdict**: ðŸŸ¢ **Different purpose, not inferior purpose**.

---

## 6. Metrics & Evaluation

### RAGBench (TRACe)
| Metric | What It Measures |
|--------|------------------|
| Context Relevance | Fraction of retrieved context relevant to query |
| Context Utilization | Fraction of retrieved context used by generator |
| Completeness | Fraction of relevant info actually used |
| Adherence | Boolean hallucination detection |

- **Token-level**: Span-based annotations, not sentence-level.
- **Trained evaluator**: Fine-tuned RoBERTa outperforms LLM judges.

### SCARF
- Uses **RAGAS metrics** (same as RAGLab), plus BLEU, ROUGE, BERTScore.
- LLM-as-Judge (GPT-Score).

### BenchmarkQED (AutoE)
| Metric | What It Measures |
|--------|------------------|
| Comprehensiveness | Addresses all relevant aspects |
| Diversity | Presents varied perspectives |
| Empowerment | Enables informed judgments |
| Relevance | Addresses specific query aspects |

- **Pairwise comparison**: Win rates between systems, not absolute scores.
- **No ground truth**: Evaluation without reference answers.

### RAGLab
| Metric | Source |
|--------|--------|
| Faithfulness | RAGAS |
| Relevancy | RAGAS |
| Context Precision | RAGAS |
| Context Recall | RAGAS |
| Answer Correctness | RAGAS |

**Assessment**:
- RAGLab uses standard RAGAS metrics, same as SCARF.
- RAGBench's TRACe is more granular (span-level).
- BenchmarkQED's pairwise comparison is interesting for method comparison.
- RAGLab's ground truth quotes enable **true** context_recall evaluation (most benchmarks lack this).

**Verdict**: ðŸŸ¡ **Standard approach with good ground truth**. Consider adding TRACe-style utilization metrics.

---

## 7. Professional Quality & Coverage

### Honest Assessment

| Dimension | Professional Frameworks | RAGLab |
|-----------|------------------------|--------|
| **Statistical Power** | 1k-100k samples | 15-45 samples |
| **Reproducibility** | Fixed public datasets | Single private corpus |
| **External Validity** | Multi-domain | Two niche domains |
| **Implementation Depth** | Evaluation-focused | Full pipeline |
| **Documentation** | Academic papers | CLAUDE.md + memory-bank |
| **Community** | Public datasets, benchmarks | Personal use |

### What RAGLab Has That They Don't

1. **End-to-end pipeline**: Built extraction â†’ chunking â†’ embedding â†’ retrieval â†’ generation. Benchmarks assume this exists.

2. **Research implementation**: Implemented HyDE, decomposition, RAPTOR, GraphRAG from papers. Benchmarks just compare outputs.

3. **Cross-domain synthesis**: Questions require integrating Sapolsky with Epictetus. No benchmark tests this.

4. **Iterative learning**: Memory-bank tracks decisions. This is metacognition about RAG.

### What RAGLab Is Missing

1. **Sample size**: 15 questions cannot detect 5% performance differences. Need 100+ for statistical significance.

2. **External validation**: Corpus is private. No one can reproduce results.

3. **Baseline comparisons**: Can't claim "RAGLab outperforms X" without running on standard datasets.

4. **Cost/latency tracking**: None of the metrics track API costs or response timeâ€”important for production.

---

## 8. Recommendations

### If Goal is Learning (Current)
RAGLab is **excellent**. The 5D grid, research implementations, and cross-domain questions demonstrate understanding.

### If Goal is Publishing
1. **Add standard datasets**: Run on PubMedQA or MS Marco subset (available in RAGBench).
2. **Expand questions**: 50+ minimum for statistical claims.
3. **Track costs**: Add token counting and latency metrics.

### If Goal is Production
1. **Add SCARF-style black-box testing**: Test against Cheshire Cat or LangChain baselines.
2. **Add BenchmarkQED's pairwise comparison**: More robust than absolute scores.
3. **Add TRACe utilization**: Understand if retrieved context is actually used.

---

## Summary Table

| Dimension | RAGBench | SCARF | BenchmarkQED | RAGLab | Winner |
|-----------|----------|-------|--------------|--------|--------|
| Document Processing | Variable | Framework-agnostic | 200/600 tokens | Docling + 4 strategies | **RAGLab** |
| Question Generation | GPT-4 labels | Existing | AutoQ synthetic | Human + Claude | **RAGBench** (scale) / **RAGLab** (quality) |
| Dataset Scale | 100k | Varies | 1,400 articles | 19 books | **RAGBench** |
| Hyperparameter Grid | None | Framework-level | 4-8 configs | 100+ combos | **RAGLab** |
| Metrics | TRACe (custom) | RAGAS + BLEU | Pairwise | RAGAS | **RAGBench** |
| Purpose | Benchmark creation | Prod testing | GraphRAG research | Learning | N/A |

---

## Final Honest Take

**RAGLab is a legitimate RAG research platform**, not a toy. The frameworks reviewed are:
- **RAGBench**: For training custom evaluators with 100k labeled examples
- **SCARF**: For comparing deployed RAG products
- **BenchmarkQED**: For proving GraphRAG > vector RAG

**RAGLab is for understanding RAG by building it**. That's a different goal, and it succeeds at it.

The main gaps are scale (15 vs. 100k questions) and external validation (private corpus). But the hyperparameter exploration, research implementations, and cross-domain synthesis testing are **better than what these benchmarks do**.

To publish, would need to run on standard datasets. For learning, RAGLab is ahead of most tutorials and many papers.

---

## References

- [RAGBench Paper (arXiv:2407.11005)](https://arxiv.org/abs/2407.11005)
- [RAGBench Dataset (HuggingFace)](https://huggingface.co/datasets/rungalileo/ragbench)
- [SCARF Paper (arXiv:2504.07803)](https://arxiv.org/abs/2504.07803)
- [SCARF GitHub](https://github.com/Eustema-S-p-A/SCARF)
- [BenchmarkQED Microsoft Blog](https://www.microsoft.com/en-us/research/blog/benchmarkqed-automated-benchmarking-of-rag-systems/)
- [BenchmarkQED GitHub](https://github.com/microsoft/benchmark-qed)
