{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f815780-6a22-4bbd-b058-98888bd19fcb",
   "metadata": {},
   "source": [
    "# clean markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b2f173-996e-4749-b5a4-8f82c498e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9b6225-9147-419d-9c1f-ce5b3e154403",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = Path.cwd()\n",
    "PROJECT_ROOT = notebook_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f3ef1f-0dd9-4f17-9613-e9ea250f707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"PRECLEAN_Brain_and_behavior_a_cognitive_neuroscience_perspective_David_Eagleman_Jonathan_Downar\"\n",
    "filename_removedpictures = filename + \"_removedpictures\"\n",
    "destination_file_removedpictures= (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removedpictures).with_suffix(\".md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0930f5b-c7d4-4b0d-8ab8-b2e3cb589405",
   "metadata": {},
   "source": [
    "## Helper Functions (The Logic Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f893d55d-6a60-4957-a478-4157b1baca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REGEX PATTERNS FOR WHOLE LINES ---\n",
    "LINE_ARTIFACT_PATTERNS = [\n",
    "    (r'^\\s*(FIGURE|FIG|Fig|TABLE|TAB|Tab)(\\.?)\\s*[\\d\\.\\-]+\\s*.*$', ''),\n",
    "    (r'^\\s*(Source|Credit|Data from):.*$', ''),\n",
    "]\n",
    "\n",
    "# --- PATTERNS FOR INLINE REMOVAL ---\n",
    "INLINE_REMOVAL_PATTERNS = [\n",
    "    r'\\(\\s*(FIGURE|FIG|Fig|TABLE|TAB|Tab)\\.?\\s*[\\d\\.\\-]+\\s*\\)',\n",
    "]\n",
    "\n",
    "def clean_whole_lines(text):\n",
    "    \"\"\"\n",
    "    Removes lines that are likely image or table captions using Regex.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        skip = False\n",
    "        for pattern, replacement in LINE_ARTIFACT_PATTERNS:\n",
    "            if re.search(pattern, line, flags=re.IGNORECASE):\n",
    "                skip = True\n",
    "                break\n",
    "        if not skip:\n",
    "            cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def fix_broken_chapters(text):\n",
    "    \"\"\"\n",
    "    Fixes broken chapter headers. Handles two specific patterns:\n",
    "    \n",
    "    Pattern 1 (Numbered):\n",
    "      CHAPTER\n",
    "      ## Motivation and Reward\n",
    "      14\n",
    "      -> # CHAPTER 14 Motivation and Reward\n",
    "      \n",
    "    Pattern 2 (Intro/Unnumbered):\n",
    "      ## CHAPTER\n",
    "      ## Introduction\n",
    "      -> # CHAPTER Introduction\n",
    "    \"\"\"\n",
    "    # PATTERN 1: Numbered Chapters\n",
    "    # Matches \"CHAPTER\" (no hashes), then \"## Title\", then \"Number\"\n",
    "    p1 = r'(?m)^\\s*CHAPTER\\s*$\\n+\\s*^##\\s*(.+?)\\s*$\\n+\\s*^(\\d+)\\s*$'\n",
    "    text = re.sub(p1, r'# CHAPTER \\2 \\1', text)\n",
    "    \n",
    "    # PATTERN 2: Unnumbered/Intro Chapters\n",
    "    # Matches \"## CHAPTER\", then \"## Title\"\n",
    "    # (?m) = Multiline mode\n",
    "    # ^\\s*##\\s*CHAPTER\\s*$ = Line with exactly \"## CHAPTER\"\n",
    "    # \\n+ = Newlines\n",
    "    # ^\\s*##\\s*(.+?)\\s*$ = Line with \"## Title\", capturing the title\n",
    "    p2 = r'(?m)^\\s*##\\s*CHAPTER\\s*$\\n+\\s*^##\\s*(.+?)\\s*$'\n",
    "    text = re.sub(p2, r'# CHAPTER \\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_inline_formatting(text):\n",
    "    \"\"\"\n",
    "    Refines string spacing and removes specific OCR garbage WITHIN sentences.\n",
    "    \"\"\"\n",
    "    # 1. Fix /u2014 artifacts\n",
    "    text = text.replace('/u2014.d', ' - ')\n",
    "    text = text.replace('/u2014', ' - ') \n",
    "    \n",
    "    # 2. Remove inline patterns\n",
    "    for pattern in INLINE_REMOVAL_PATTERNS:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3. Fix 'mu- opioid' -> 'mu-opioid'\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1-\\2', text)\n",
    "    \n",
    "    # 4. Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def consolidate_broken_paragraphs(paragraphs):\n",
    "    \"\"\"\n",
    "    CRITICAL FUNCTION: Fixes blank lines interrupting sentences.\n",
    "    \"\"\"\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "    \n",
    "    merged = []\n",
    "    buffer = paragraphs[0].strip()\n",
    "    \n",
    "    for i in range(1, len(paragraphs)):\n",
    "        current_p = paragraphs[i].strip()\n",
    "        if not current_p:\n",
    "            continue\n",
    "            \n",
    "        ends_with_terminal = buffer.endswith(('.', '!', '?', ':', ';', '\"', 'â€'))\n",
    "        starts_lowercase = current_p[0].islower() if len(current_p) > 0 else False\n",
    "        ends_connector = buffer.endswith((',', '-'))\n",
    "\n",
    "        if (not ends_with_terminal) or starts_lowercase or ends_connector:\n",
    "            buffer += \" \" + current_p\n",
    "        else:\n",
    "            merged.append(buffer)\n",
    "            buffer = current_p\n",
    "            \n",
    "    if buffer:\n",
    "        merged.append(buffer)\n",
    "        \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110665c-16c6-45e9-a361-a9605478e0cc",
   "metadata": {},
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbbff66-a3d1-45cb-8452-82281ebdf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step_1_manual_cleaning(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        md_content = f.read()\n",
    "\n",
    "    # --- CRITICAL FIX: Run the Chapter Stitcher FIRST ---\n",
    "    # This runs on the raw text to catch the multi-line chapter pattern\n",
    "    md_content = fix_broken_chapters(md_content)\n",
    "\n",
    "    # 1. Global Line Removal (Captions)\n",
    "    clean_content = clean_whole_lines(md_content)\n",
    "    \n",
    "    # 2. Split by Headers (#) to preserve document structure\n",
    "    sections = re.split(r'(^#+\\s+.*$)', clean_content, flags=re.MULTILINE)\n",
    "    \n",
    "    reconstructed_text = []\n",
    "    \n",
    "    for segment in sections:\n",
    "        segment = segment.strip()\n",
    "        if not segment: continue\n",
    "            \n",
    "        # If it's a Header, just add it to our output list\n",
    "        if segment.startswith('#'):\n",
    "            reconstructed_text.append(f\"\\n\\n{segment}\\n\\n\")\n",
    "            continue\n",
    "            \n",
    "        # If it's Body Text:\n",
    "        raw_paragraphs = segment.split('\\n\\n')\n",
    "        stitched_paragraphs = consolidate_broken_paragraphs(raw_paragraphs)\n",
    "        \n",
    "        for p in stitched_paragraphs:\n",
    "            reflowed = p.replace('\\n', ' ')\n",
    "            final_p = clean_inline_formatting(reflowed)\n",
    "            \n",
    "            if len(final_p) > 0:\n",
    "                reconstructed_text.append(f\"{final_p}\\n\\n\")\n",
    "                \n",
    "    return \"\".join(reconstructed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc16dd2-969c-46d9-bc77-ca3af21ddd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removedpictures).with_suffix(\".md\")\n",
    "# File 1: The result of Manual Cleaning (Regex + Paragraph Stitching)\n",
    "output_manual_clean = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / (filename_removedpictures + \"_STEP1_manual\")).with_suffix(\".md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325eba1a-aa5e-45ea-8910-15bae05d171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1 COMPLETE. Cleaned Markdown saved to: /home/fliperbaker/projects/rag1/rag1-mini/data/processed/neuroscience/PRECLEAN_Brain_and_behavior_a_cognitive_neuroscience_perspective_David_Eagleman_Jonathan_Downar_removedpictures_STEP1_manual.md\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION OF STEP 1 ---\n",
    "cleaned_markdown_text = run_step_1_manual_cleaning(input_file_path)\n",
    "\n",
    "# Save to file\n",
    "with open(output_manual_clean, 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_markdown_text)\n",
    "\n",
    "print(f\"STEP 1 COMPLETE. Cleaned Markdown saved to: {output_manual_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ff584-4bc0-466e-889e-15c03c7f104b",
   "metadata": {},
   "source": [
    "## spacy sentence separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cbd7359-3d78-4263-854d-d3d23d21cfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciSpaCy model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fliperbaker/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the Scientific SpaCy model\n",
    "# disable=[\"ner\"] makes it faster; we only need the parser for sentence boundaries.\n",
    "print(\"Loading SciSpaCy model...\")\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b15764e-31f5-4940-903d-cb7af0c3aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step_2_spacy_processing(clean_text):\n",
    "    \"\"\"\n",
    "    Takes already cleaned text, parses it with SciSpaCy, \n",
    "    and adds metadata (Context tracking and Sentence counts).\n",
    "    \n",
    "    CRITICAL CHANGE: Now saves the 'sentences' list to preserve \n",
    "    SciSpaCy's intelligent splitting for the next phase.\n",
    "    \"\"\"\n",
    "    # Split by Header again to determine context\n",
    "    sections = re.split(r'(^#+\\s+.*$)', clean_text, flags=re.MULTILINE)\n",
    "    \n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Context trackers\n",
    "    current_chapter = \"Unknown Chapter\"\n",
    "    current_section = \"\"\n",
    "    \n",
    "    for segment in sections:\n",
    "        segment = segment.strip()\n",
    "        if not segment: continue\n",
    "            \n",
    "        # Update Context if we hit a header\n",
    "        if segment.startswith('#'):\n",
    "            clean_header = segment.lstrip('#').strip()\n",
    "            if segment.startswith('# '):\n",
    "                current_chapter = clean_header\n",
    "                current_section = \"\" # Reset section on new chapter\n",
    "            elif segment.startswith('##'):\n",
    "                current_section = clean_header\n",
    "            continue\n",
    "            \n",
    "        # Process Body Text\n",
    "        # Note: Step 1 already handled the stitching and inline cleaning.\n",
    "        # We just split by double newline here to get the \"Paragraph Blocks\".\n",
    "        paragraphs = segment.split('\\n\\n')\n",
    "        \n",
    "        for p in paragraphs:\n",
    "            p = p.strip()\n",
    "            if not p: continue\n",
    "                \n",
    "            # --- SPACY NLP STEP ---\n",
    "            # We pass the text to SpaCy to validate and split sentences intelligently.\n",
    "            doc = nlp(p)\n",
    "            \n",
    "            # Extract the smart splits\n",
    "            # This preserves the logic that \"et al.\" is NOT a sentence break.\n",
    "            sentence_list = [sent.text.strip() for sent in doc.sents]\n",
    "            \n",
    "            # Construct context string\n",
    "            context_string = f\"{current_chapter}\"\n",
    "            if current_section:\n",
    "                context_string += f\" > {current_section}\"\n",
    "            \n",
    "            chunk_data = {\n",
    "                \"context\": context_string,\n",
    "                # We save the full text for reference\n",
    "                \"text\": p,\n",
    "                # CRITICAL: We save the list of sentences for the Embedding Step\n",
    "                \"sentences\": sentence_list,\n",
    "                \"num_sentences\": len(sentence_list)\n",
    "            }\n",
    "            processed_chunks.append(chunk_data)\n",
    "            \n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b0858b-b02f-4889-b771-972b9a1e0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File 2: The result of SpaCy Processing (Metadata + Sentence Counts)\n",
    "output_spacy_processed = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / (filename_removedpictures + \"_STEP2_spacy\")).with_suffix(\".md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08636b6-5517-4652-b1f1-2f3c43614816",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m output_lines.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Visual check: Print sentences as a bulleted list to verify splits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentences\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m     17\u001b[39m     output_lines.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m output_lines.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'sentences'"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION OF STEP 2 ---\n",
    "# We use the 'cleaned_markdown_text' variable generated in Cell 4\n",
    "final_chunks = run_step_2_spacy_processing(cleaned_markdown_text)\n",
    "\n",
    "# Format for the final Markdown file (Human Readable Chunk View)\n",
    "output_lines = [f\"# Spacy Processed Analysis: {filename}\\n\"]\n",
    "\n",
    "for i, chunk in enumerate(final_chunks):\n",
    "    output_lines.append(f\"---\") \n",
    "    output_lines.append(f\"### Chunk {i+1}\")\n",
    "    output_lines.append(f\"**Context:** `{chunk['context']}`\")\n",
    "    output_lines.append(f\"**Sentences:** {chunk['num_sentences']}\")\n",
    "    output_lines.append(f\"\") \n",
    "    \n",
    "    # Visual check: Print sentences as a bulleted list to verify splits\n",
    "    for sent in chunk['sentences']:\n",
    "        output_lines.append(f\"- {sent}\")\n",
    "        \n",
    "    output_lines.append(f\"\\n\")\n",
    "\n",
    "# Save to file\n",
    "with open(output_spacy_processed, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"STEP 2 COMPLETE. Processed Analysis saved to: {output_spacy_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a2bf6-94d0-4fa7-81be-7180d44882e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1-mini",
   "language": "python",
   "name": "rag1-mini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
