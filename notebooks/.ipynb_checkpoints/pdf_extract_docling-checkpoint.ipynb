{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d4f10a-38ea-4396-828f-5b6c0ac4c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42854987-6fb0-4783-b7fa-020a73914a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports to support OCR configuration\n",
    "from docling.datamodel.document import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import ConversionStatus\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fe2292-95ba-46f7-9a7a-4be1d6f7ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = Path.cwd()\n",
    "PROJECT_ROOT = notebook_dir.parent\n",
    "filename = \"ch1_ch14_Brain_and_behavior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5847a40-6bd6-43fc-9b4f-8af583686e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is in: /home/ccrs70/projects/rag1-mini/notebooks\n",
      "Project root is: /home/ccrs70/projects/rag1-mini\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook is in: {notebook_dir}\")\n",
    "print(f\"Project root is: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf1d9f7-588f-4e8d-a8cf-c7a3290fd26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full path and then add the .pdf suffix\n",
    "source_file = (PROJECT_ROOT / \"data\" / \"raw\" / \"neuroscience\" / filename).with_suffix(\".pdf\")\n",
    "\n",
    "# Construct the full path and then add the .md suffix\n",
    "destination_file = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename).with_suffix(\".md\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c36be-d907-4f24-840c-d59ddd4d004a",
   "metadata": {},
   "source": [
    "## conversion no_ocr no_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f85002e-14cd-441e-97b6-d4f0d9326341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Pipeline Options\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = False           # <--- Force OCR OFF (Fast & clean for digital docs)\n",
    "pipeline_options.do_table_structure = False # Keep table recognition ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de813c95-4fa7-411e-8824-92af8985e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize Converter with specific PDF options\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7516cea-edd9-4190-8197-41e9baa1a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 09:08:07,771 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-03 09:08:07,876 - INFO - Going to convert document batch...\n",
      "2025-12-03 09:08:07,877 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 1216607fb7e04989285a12764e030fc9\n",
      "2025-12-03 09:08:07,893 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-03 09:08:07,897 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-03 09:08:07,910 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-03 09:08:07,918 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-03 09:08:07,929 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-03 09:08:09,048 - INFO - Processing document ch1_ch14_Brain_and_behavior.pdf\n",
      "2025-12-03 09:10:03,691 - INFO - Finished converting document ch1_ch14_Brain_and_behavior.pdf in 115.92 sec.\n"
     ]
    }
   ],
   "source": [
    "result = converter.convert(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc8a61b-35f6-4aa1-ad86-6a6ff8ae5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the internal DoclingDocument object\n",
    "doc = result.document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d786ec5-fa33-4d5a-851b-ad9ff66ed8a1",
   "metadata": {},
   "source": [
    "## save markdown and json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9a4dfc-09f8-4e4d-a5a9-9908a29e7b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_file.write_text(result.document.export_to_markdown(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350ee2a2-4212-467b-b27c-03d664e63376",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_json = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename).with_suffix(\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4917757-7bb1-4f24-8688-e8db106c7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Option B: Save to a JSON file for easier inspection in a text editor\n",
    "doc.save_as_json(destination_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78790ae-6f97-40bb-a036-fca86efa03de",
   "metadata": {},
   "source": [
    "## possible labels of docling items\n",
    "https://github.com/docling-project/docling-core/blob/main/docling_core/types/doc/labels.py\n",
    "```\n",
    "class DocItemLabel(str, Enum):\n",
    "    \"\"\"DocItemLabel.\"\"\"\n",
    "\n",
    "    CAPTION = \"caption\"\n",
    "    CHART = \"chart\"\n",
    "    FOOTNOTE = \"footnote\"\n",
    "    FORMULA = \"formula\"\n",
    "    LIST_ITEM = \"list_item\"\n",
    "    PAGE_FOOTER = \"page_footer\"\n",
    "    PAGE_HEADER = \"page_header\"\n",
    "    PICTURE = \"picture\"\n",
    "    SECTION_HEADER = \"section_header\"\n",
    "    TABLE = \"table\"\n",
    "    TEXT = \"text\"\n",
    "    TITLE = \"title\"\n",
    "    DOCUMENT_INDEX = \"document_index\"\n",
    "    CODE = \"code\"\n",
    "    CHECKBOX_SELECTED = \"checkbox_selected\"\n",
    "    CHECKBOX_UNSELECTED = \"checkbox_unselected\"\n",
    "    FORM = \"form\"\n",
    "    KEY_VALUE_REGION = \"key_value_region\"\n",
    "    GRADING_SCALE = \"grading_scale\"  # for elements in forms, questionaires representing a grading scale\n",
    "    # e.g. [strongly disagree | ... | ... | strongly agree]\n",
    "    # e.g. ★★☆☆☆\n",
    "    HANDWRITTEN_TEXT = \"handwritten_text\"\n",
    "    EMPTY_VALUE = \"empty_value\"  # used for empty value fields in fillable forms\n",
    "\n",
    "    # Additional labels for markup-based formats (e.g. HTML, Word)\n",
    "    PARAGRAPH = \"paragraph\"\n",
    "    REFERENCE = \"reference\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b7ec1f-42de-4401-8e1c-623cba6d7c37",
   "metadata": {},
   "source": [
    "## delete unwanted items\n",
    "\n",
    "DocItemLabel.CAPTION, DocItemLabel.FOOTNOTE, \n",
    "                    DocItemLabel.PAGE_FOOTER, DocItemLabel.PAGE_HEADER, \n",
    "                    DocItemLabel.TABLE\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "630cfcb7-0f62-458f-87d4-f774592cf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.document import DocItemLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b36db0-ecd5-4b57-87bf-de4a1564417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Identify items to remove (e.g., remove all Captions and Footnotes)\n",
    "# We collect them in a list first to avoid modifying the tree while iterating\n",
    "items_to_remove = []\n",
    "labels_to_remove = {DocItemLabel.CAPTION, DocItemLabel.FOOTNOTE, \n",
    "                    DocItemLabel.PAGE_FOOTER, DocItemLabel.PAGE_HEADER, \n",
    "                    DocItemLabel.TABLE}\n",
    "\n",
    "for item, level in doc.iterate_items():\n",
    "    # Check if the item has a label and if it matches our target list\n",
    "    if hasattr(item, \"label\") and item.label in labels_to_remove:\n",
    "        items_to_remove.append(item)\n",
    "\n",
    "# 3. Delete the items from the document\n",
    "# This updates the document tree in-place\n",
    "doc.delete_items(node_items=items_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1655fa37-9d30-49b2-ac7e-fb6c0dc9b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_removeditems = filename + \"_removeditems\"\n",
    "destination_file_removeditems = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removeditems).with_suffix(\".md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833b2929-1b82-49ed-84e7-2d5584ed750a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242838"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Now 'doc' can be passed to your RAG pipeline (e.g., chunking or export)\n",
    "destination_file_removeditems.write_text(doc.export_to_markdown(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8178c03f-889a-462c-9baa-2bbf4b2928a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_file_json_removeditems = (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removeditems).with_suffix(\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c7172c4-e678-4270-b099-7b287aea120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.save_as_json(destination_file_json_removeditems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827027d-4248-476e-bf04-cf2110e809e9",
   "metadata": {},
   "source": [
    "## delete pictures and its children\n",
    "removes picture elements that contain texts inside the picture. No OCR is done but small texts like (a) (b) digital text inside the picture makes the converter to create picture items (empty) that contains small texts. This is to remove that noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3feaa2b5-6154-4851-808e-5d63bcab3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Helper function to find all children recursively\n",
    "def get_all_descendants(item):\n",
    "    \"\"\"Recursively collect all children, grandchildren, etc.\"\"\"\n",
    "    descendants = []\n",
    "    # Check if the item has children\n",
    "    if hasattr(item, \"children\") and item.children:\n",
    "        for child in item.children:\n",
    "            descendants.append(child)\n",
    "            # Recursively get children of the child\n",
    "            descendants.extend(get_all_descendants(child))\n",
    "    return descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "965d60fd-0f72-4bda-8d33-f159d4b60bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 651 items...\n"
     ]
    }
   ],
   "source": [
    "# 3. Identify items to remove\n",
    "items_to_remove = []   # Use a list instead of a set\n",
    "seen_ids = set()       # Track IDs to avoid duplicates\n",
    "\n",
    "for item, level in doc.iterate_items():\n",
    "    # Check if it is a Picture\n",
    "    if hasattr(item, \"label\") and item.label == DocItemLabel.PICTURE:\n",
    "        \n",
    "        # A. Add the Picture item itself (if not already added)\n",
    "        if id(item) not in seen_ids:\n",
    "            items_to_remove.append(item)\n",
    "            seen_ids.add(id(item))\n",
    "        \n",
    "        # B. Get all children (captions, texts inside)\n",
    "        children = get_all_descendants(item)\n",
    "        \n",
    "        for child in children:\n",
    "            if id(child) not in seen_ids:\n",
    "                items_to_remove.append(child)\n",
    "                seen_ids.add(id(child))\n",
    "\n",
    "# 4. Delete the items\n",
    "if items_to_remove:\n",
    "    print(f\"Removing {len(items_to_remove)} items...\")\n",
    "    doc.delete_items(node_items=items_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a9428d3-f924-413c-bfa8-f74035585753",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_removedpictures = filename + \"_removedpictures\"\n",
    "destination_file_json_removedpictures= (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removedpictures).with_suffix(\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79cdb282-0e8e-48f0-879d-266bd52557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.save_as_json(destination_file_json_removedpictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5341cd90-e689-407d-9649-6b02502cdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_file_removedpictures= (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removedpictures).with_suffix(\".md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "451334d1-3ba7-4e41-93d9-31d4df5dcaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241622"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_file_removedpictures.write_text(doc.export_to_markdown(), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb001a06-de34-4642-9d8f-c793a2ba304e",
   "metadata": {},
   "source": [
    "## scispacy to sementation, cleaning and named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40dd1ba9-60c3-4571-92ed-d0211fce42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bb9a2ea-321c-43d3-9cb2-7ba1b15d759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION & SETUP ---\n",
    "# We define a data structure to keep our data clean as it moves through the pipeline\n",
    "@dataclass\n",
    "class ProcessedSentence:\n",
    "    text: str                     # The actual sentence string\n",
    "    entities: List[str]           # List of scientific entities found (e.g., \"Hippocampus\")\n",
    "    entity_labels: List[str]      # The types of entities (e.g., \"ORGAN\", \"CELL\")\n",
    "    source_metadata: Dict[str, Any] # The book info (Title, Author, Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddc057be-ea62-4e0d-b0f3-3e203e04f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuroProcessor:\n",
    "    def __init__(self):\n",
    "        print(\"⏳ Loading SciSpacy models... (This happens only once)\")\n",
    "        \n",
    "        # MODEL 1: Structural/Linguistic Model\n",
    "        # Used for: Accurate Sentence Splitting\n",
    "        try:\n",
    "            self.nlp_seg = spacy.load(\"en_core_sci_md\")\n",
    "        except OSError:\n",
    "            raise OSError(\"Could not load 'en_core_sci_md'. Did you install it?\")\n",
    "\n",
    "        # MODEL 2: Specialized NER Model\n",
    "        # Used for: Extracting specific BioNLP entities\n",
    "        try:\n",
    "            self.nlp_ner = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "        except OSError:\n",
    "            raise OSError(\"Could not load 'en_ner_bionlp13cg_md'. Did you install it?\")\n",
    "            \n",
    "        print(\"✅ Models loaded successfully.\")\n",
    "\n",
    "    def process_text(self, raw_text: str, source_metadata: Dict[str, Any]) -> List[ProcessedSentence]:\n",
    "        \"\"\"\n",
    "        Takes raw text and global metadata, returns a list of enriched sentence objects.\n",
    "        \"\"\"\n",
    "        # 1. Basic Cleaning\n",
    "        # PDF text often has line breaks in the middle of sentences. We flatten them.\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "        \n",
    "        print(f\"   -> Processing text length: {len(cleaned_text)} chars\")\n",
    "\n",
    "        # 2. Segmentation (Splitting into sentences)\n",
    "        # We use the 'sci' model because it handles abbreviations (e.g., 'Fig. 1', 'et al.') better\n",
    "        doc = self.nlp_seg(cleaned_text)\n",
    "        \n",
    "        # Filter: Ignore tiny sentences (< 10 chars) usually artifacts/page numbers\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "        print(f\"   -> Found {len(sentences)} valid sentences.\")\n",
    "\n",
    "        processed_output = []\n",
    "\n",
    "        # 3. Enrichment (NER + Stamping)\n",
    "        for sent in sentences:\n",
    "            # Run the Specialized NER model on just this sentence\n",
    "            ner_doc = self.nlp_ner(sent)\n",
    "            \n",
    "            # Extract unique entities\n",
    "            entities = list(set([ent.text for ent in ner_doc.ents]))\n",
    "            entity_types = list(set([ent.label_ for ent in ner_doc.ents]))\n",
    "            \n",
    "            # Create the object\n",
    "            processed_sentence = ProcessedSentence(\n",
    "                text=sent,\n",
    "                entities=entities,\n",
    "                entity_labels=entity_types,\n",
    "                source_metadata=source_metadata # <--- Metadata Injection\n",
    "            )\n",
    "            processed_output.append(processed_sentence)\n",
    "            \n",
    "        return processed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb1ff9f3-0114-41d5-bbde-79c001cc3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading SciSpacy models... (This happens only once)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccrs70/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/util.py:969: UserWarning: [W095] Model 'en_core_sci_md' (0.5.4) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.8.11). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy.pipeline.factories'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- INITIALIZE PROCESSOR ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m processor = \u001b[43mNeuroProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mNeuroProcessor.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# MODEL 1: Structural/Linguistic Model\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Used for: Accurate Sentence Splitting\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mself\u001b[39m.nlp_seg = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_sci_md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not load \u001b[39m\u001b[33m'\u001b[39m\u001b[33men_core_sci_md\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Did you install it?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/util.py:524\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model_from_path\u001b[39m(\n\u001b[32m    505\u001b[39m     model_path: Path,\n\u001b[32m    506\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    512\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = SimpleFrozenDict(),\n\u001b[32m    513\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLanguage\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    514\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a model from a data directory path. Creates Language class with\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[33;03m    pipeline from config.cfg and then calls from_disk() with path.\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03m    model_path (Path): Model path.\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[33;03m    meta (Dict[str, Any]): Optional model meta.\u001b[39;00m\n\u001b[32m    519\u001b[39m \u001b[33;03m    vocab (Vocab / True): Optional vocab to pass in on initialization. If True,\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[33;03m        a new Vocab object will be created.\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[33;03m    disable (Union[str, Iterable[str]]): Name(s) of pipeline component(s) to disable. Disabled\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[33;03m        pipes will be loaded but they won't be run unless you explicitly\u001b[39;00m\n\u001b[32m    523\u001b[39m \u001b[33;03m        enable them by calling nlp.enable_pipe.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[33;03m    enable (Union[str, Iterable[str]]): Name(s) of pipeline component(s) to enable. All other\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[33;03m        pipes will be disabled (and can be enabled using `nlp.enable_pipe`).\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[33;03m    exclude (Union[str, Iterable[str]]): Name(s) of pipeline component(s) to exclude. Excluded\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m        components won't be loaded.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m    config (Dict[str, Any] / Config): Config overrides as nested dict or dict\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m        keyed by section values in dot notation.\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path.exists():\n\u001b[32m    533\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E052.format(path=model_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/util.py:560\u001b[39m, in \u001b[36mload_model_from_package\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    539\u001b[39m     nlp = load_model_from_config(\n\u001b[32m    540\u001b[39m         config,\n\u001b[32m    541\u001b[39m         vocab=vocab,\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m         meta=meta,\n\u001b[32m    546\u001b[39m     )\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model_from_config\u001b[39m(\n\u001b[32m    551\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config],\n\u001b[32m    552\u001b[39m     *,\n\u001b[32m    553\u001b[39m     meta: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = SimpleFrozenDict(),\n\u001b[32m    554\u001b[39m     vocab: Union[\u001b[33m\"\u001b[39m\u001b[33mVocab\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    555\u001b[39m     disable: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]] = _DEFAULT_EMPTY_PIPES,\n\u001b[32m    556\u001b[39m     enable: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]] = _DEFAULT_EMPTY_PIPES,\n\u001b[32m    557\u001b[39m     exclude: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]] = _DEFAULT_EMPTY_PIPES,\n\u001b[32m    558\u001b[39m     auto_fill: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    559\u001b[39m     validate: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLanguage\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    561\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create an nlp object from a config. Expects the full config file including\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03m    a section \"nlp\" containing the settings for the nlp object.\u001b[39;00m\n\u001b[32m    563\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    577\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/en_core_sci_md/__init__.py:10\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(**overrides)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(**overrides):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/util.py:741\u001b[39m, in \u001b[36mload_model_from_init_py\u001b[39m\u001b[34m(init_file, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/util.py:598\u001b[39m, in \u001b[36mload_model_from_path\u001b[39m\u001b[34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/util.py:646\u001b[39m, in \u001b[36mload_model_from_config\u001b[39m\u001b[34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[39m\n\u001b[32m    644\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnot a valid section reference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    645\u001b[39m             errors.append({\u001b[33m\"\u001b[39m\u001b[33mloc\u001b[39m\u001b[33m\"\u001b[39m: name.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m: msg})\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigValidationError(config=config, errors=errors)\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/language.py:1857\u001b[39m, in \u001b[36mfrom_config\u001b[39m\u001b[34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[39m\n\u001b[32m   1849\u001b[39m warn_if_jupyter_cupy()\n\u001b[32m   1851\u001b[39m \u001b[38;5;66;03m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[32m   1852\u001b[39m \u001b[38;5;66;03m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[32m   1853\u001b[39m \u001b[38;5;66;03m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[32m   1854\u001b[39m \u001b[38;5;66;03m# and then again when we load from disk.\u001b[39;00m\n\u001b[32m   1855\u001b[39m nlp = lang_cls(\n\u001b[32m   1856\u001b[39m     vocab=vocab,\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     create_tokenizer=create_tokenizer,\n\u001b[32m   1858\u001b[39m     create_vectors=create_vectors,\n\u001b[32m   1859\u001b[39m     meta=meta,\n\u001b[32m   1860\u001b[39m )\n\u001b[32m   1861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m after_creation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1862\u001b[39m     nlp = after_creation(nlp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/language.py:186\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, vocab, max_length, meta, create_tokenizer, create_vectors, batch_size, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m def __init__(\n\u001b[32m    156\u001b[39m     self,\n\u001b[32m    157\u001b[39m     vocab: Union[Vocab, bool] = True,\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     **kwargs,\n\u001b[32m    165\u001b[39m ) -> None:\n\u001b[32m    166\u001b[39m     \"\"\"Initialise a Language object.\n\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m     vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m     DOCS: https://spacy.io/api/language#init\n\u001b[32m    184\u001b[39m     \"\"\"\n\u001b[32m    185\u001b[39m     # We're only calling this to import all factories provided via entry\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     # points. The factory decorator applied to these functions takes care\n\u001b[32m    187\u001b[39m     # of the rest.\n\u001b[32m    188\u001b[39m     util.registry._entry_point_factories.get_all()\n\u001b[32m    190\u001b[39m     self._config = DEFAULT_CONFIG.merge(self.default_config)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy.pipeline.factories'"
     ]
    }
   ],
   "source": [
    "# --- INITIALIZE PROCESSOR ---\n",
    "processor = NeuroProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6129b48-637f-4f64-90cc-51cb189c6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DEFINE METADATA\n",
    "# We define this ONCE for the whole file\n",
    "book_metadata = {\n",
    "    \"title\": \"Brain and Behavior\",\n",
    "    \"author\": \"David Eagleman\",\n",
    "    \"file_source\": \"Eagleman_Textbook.pdf\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1-mini",
   "language": "python",
   "name": "rag1-mini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
