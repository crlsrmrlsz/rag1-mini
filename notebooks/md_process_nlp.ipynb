{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7425d-bf9e-4ba6-8a35-464b96df9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5e377-7b56-4f05-a2ea-dedaa293f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = Path.cwd()\n",
    "PROJECT_ROOT = notebook_dir.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b9a49-5aa4-475f-85f6-210d909bc0cd",
   "metadata": {},
   "source": [
    "## scispacy to sementation, cleaning and named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef38da1-a288-48e6-9329-45a6bb561e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ch1_ch14_Brain_and_behavior\"\n",
    "filename_removedpictures = filename + \"_removedpictures\"\n",
    "destination_file_removedpictures= (PROJECT_ROOT / \"data\" / \"processed\" / \"neuroscience\" / filename_removedpictures).with_suffix(\".md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963595b-3c44-4a2d-9e60-06a0c668249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Model 1: `en_core_sci_sm` (General Scientific)**\n",
    "### **Model 2: `en_ner_bc5cdr_md` (Biomedical)**\n",
    "### **Model 3: `en_ner_bionlp13cg_md` (Biological Processes)**\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz\n",
    "\n",
    "# print(\"‚úÖ All libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44484c54-c0d7-4d9d-8c8a-3339e586eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.14 (main, Oct 21 2025, 18:31:21) [GCC 11.2.0]\n",
      "Python executable: /home/fliperbaker/miniconda3/envs/rag1-mini/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "912ac20d-2bb4-495a-a280-ad0f93626513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Import All Required Libraries\n",
    "import spacy  # Main NLP library\n",
    "import json   # For saving structured data\n",
    "import re     # For text pattern matching and cleaning\n",
    "from collections import defaultdict  # For organizing entities by type\n",
    "from pathlib import Path  # For file path handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ac524b8-9e25-4db0-a8f8-c6bfeb2c4fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAW MARKDOWN FILE LOADED\n",
      "======================================================================\n",
      "Book: Brain and Behaviour- Daniel Eagleman\n",
      "Total characters: 241,622\n",
      "Total lines: 1,084\n",
      "File size: 235.96 KB\n",
      "\n",
      "SAMPLE OF RAW CONTENT (first 500 characters):\n",
      "----------------------------------------------------------------------\n",
      "## CHAPTER\n",
      "\n",
      "## Introduction\n",
      "\n",
      "STARTING OUT: A Spark of Awe in the Darkness\n",
      "\n",
      "Who Are We?\n",
      "\n",
      "In Pursuit of Principles\n",
      "\n",
      "How We Know What We Know\n",
      "\n",
      "RESEARCH METHODS: Magnetic Resonance Imaging\n",
      "\n",
      "Thinking Critically about the Brain\n",
      "\n",
      "The Big Questions in Cognitive Neuroscience\n",
      "\n",
      "The Payoffs of Cognitive Neuroscience\n",
      "\n",
      "1\n",
      "\n",
      "## STARTING OUT:\n",
      "\n",
      "## A Spark of Awe in the Darkness\n",
      "\n",
      "On October 9, 1604, a brilliant spark of light grew to life in the darkness of the  night  sky  over  Europe.  A  few days later, the ast\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ö†Ô∏è  Found 112 instances of 3+ consecutive spaces\n",
      "‚ö†Ô∏è  Found 0 instances of 4+ consecutive line breaks\n",
      "‚ö†Ô∏è  Found 56 spaces before punctuation\n",
      "\n",
      "‚úÖ Raw markdown loaded and analyzed!\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Load Your Raw Markdown File\n",
    "# This is STEP 1: Raw Markdown (600 pages)\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION: Change these to match your files\n",
    "# ============================================\n",
    "MARKDOWN_FILE = destination_file_removedpictures  # Your markdown file path\n",
    "BOOK_NAME = \"Brain and Behaviour- Daniel Eagleman\"  # Name for metadata\n",
    "\n",
    "# Read the markdown file\n",
    "with open(MARKDOWN_FILE, 'r', encoding='utf-8') as f:\n",
    "    raw_markdown = f.read()\n",
    "\n",
    "# Display basic statistics about the raw file\n",
    "print(\"=\" * 70)\n",
    "print(\"RAW MARKDOWN FILE LOADED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Book: {BOOK_NAME}\")\n",
    "print(f\"Total characters: {len(raw_markdown):,}\")\n",
    "print(f\"Total lines: {len(raw_markdown.splitlines()):,}\")\n",
    "print(f\"File size: {len(raw_markdown) / 1024:.2f} KB\")\n",
    "print()\n",
    "\n",
    "# Show a sample of the raw content (first 500 characters)\n",
    "print(\"SAMPLE OF RAW CONTENT (first 500 characters):\")\n",
    "print(\"-\" * 70)\n",
    "print(raw_markdown[:500])\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "\n",
    "# Show some problematic areas (if they exist)\n",
    "# Count excessive spaces\n",
    "excessive_spaces = len(re.findall(r'\\s{3,}', raw_markdown))\n",
    "print(f\"‚ö†Ô∏è  Found {excessive_spaces:,} instances of 3+ consecutive spaces\")\n",
    "\n",
    "# Count excessive line breaks\n",
    "excessive_breaks = len(re.findall(r'\\n{4,}', raw_markdown))\n",
    "print(f\"‚ö†Ô∏è  Found {excessive_breaks:,} instances of 4+ consecutive line breaks\")\n",
    "\n",
    "# Count spaces before punctuation\n",
    "spaces_before_punct = len(re.findall(r'\\s+[.,;:!?]', raw_markdown))\n",
    "print(f\"‚ö†Ô∏è  Found {spaces_before_punct:,} spaces before punctuation\")\n",
    "\n",
    "print(\"\\n‚úÖ Raw markdown loaded and analyzed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78df344f-4be2-4035-a1b8-1cf15f19c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Clean the Text\n",
    "# This is STEP 2: Clean Text (remove noise)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean common formatting issues in markdown text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text with potential formatting issues\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # CLEANING STEP 1: Fix excessive whitespace between words\n",
    "    # Example: \"the    neuron\" ‚Üí \"the neuron\"\n",
    "    text = re.sub(r'(\\w)\\s{2,}(\\w)', r'\\1 \\2', text)\n",
    "    \n",
    "    # CLEANING STEP 2: Fix multiple consecutive line breaks\n",
    "    # Example: \"\\n\\n\\n\\n\" ‚Üí \"\\n\\n\"\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # CLEANING STEP 3: Remove spaces before punctuation\n",
    "    # Example: \"word .\" ‚Üí \"word.\"\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    \n",
    "    # CLEANING STEP 4: Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "338d3162-d5cc-4dfe-8439-753e721a6441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEXT CLEANING RESULTS\n",
      "======================================================================\n",
      "Original length: 241,622 characters\n",
      "Cleaned length: 234,757 characters\n",
      "Characters removed: 6,865\n",
      "\n",
      "BEFORE CLEANING:\n",
      "----------------------------------------------------------------------\n",
      "  has  appeared  in  our  skies  to surpass  it  since  then,  even  four centuries later.\n",
      "\n",
      "Today's astronomers would have called Kepler's star a supernova and could have told him some astonishing details about the nature of the object that captured his attention on that clear night so long  ago  ( \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "AFTER CLEANING:\n",
      "----------------------------------------------------------------------\n",
      "kies to surpass it since then,  even four centuries later.\n",
      "\n",
      "Today's astronomers would have called Kepler's star a supernova and could have told him some astonishing details about the nature of the object that captured his attention on that clear night so long ago  ( FIGURE 1.1 ).  They could have to\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ Text cleaned successfully!\n",
      "üíæ Cleaned text stored in variable: cleaned_text\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning function\n",
    "cleaned_text = clean_text(raw_markdown)\n",
    "\n",
    "# Show the improvements\n",
    "print(\"=\" * 70)\n",
    "print(\"TEXT CLEANING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original length: {len(raw_markdown):,} characters\")\n",
    "print(f\"Cleaned length: {len(cleaned_text):,} characters\")\n",
    "print(f\"Characters removed: {len(raw_markdown) - len(cleaned_text):,}\")\n",
    "print()\n",
    "\n",
    "# Compare before and after on the same sample\n",
    "sample_start = 1000  # Start at character 1000 to show a middle section\n",
    "sample_length = 300\n",
    "\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(\"-\" * 70)\n",
    "print(raw_markdown[sample_start:sample_start + sample_length])\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"AFTER CLEANING:\")\n",
    "print(\"-\" * 70)\n",
    "print(cleaned_text[sample_start:sample_start + sample_length])\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ Text cleaned successfully!\")\n",
    "print(\"üíæ Cleaned text stored in variable: cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7239ea09-b38a-4640-a1d1-6c91231f0f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciSpacy models (this may take 1-2 minutes)...\n",
      "\n",
      "üì¶ Loading Model 1: en_core_sci_sm (General Scientific)\n",
      "   - This model understands scientific text structure\n",
      "   - It will break text into sentences\n",
      "   - It recognizes general scientific terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fliperbaker/miniconda3/envs/rag1-mini/lib/python3.11/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Loaded\n",
      "\n",
      "üì¶ Loading Model 2: en_ner_bc5cdr_md (Diseases & Chemicals)\n",
      "   - Trained on biomedical literature\n",
      "   - Recognizes: diseases, symptoms, drugs, chemicals\n",
      "   ‚úì Loaded\n",
      "\n",
      "üì¶ Loading Model 3: en_ner_bionlp13cg_md (Biological Processes)\n",
      "   - Trained on molecular biology papers\n",
      "   - Recognizes: proteins, genes, cellular processes\n",
      "   ‚úì Loaded\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All 3 SciSpacy models loaded and ready!\n",
      "======================================================================\n",
      "\n",
      "MODEL CAPABILITIES:\n",
      "\n",
      "Model 1 (nlp_base) can identify:\n",
      "  - Sentences, tokens, parts of speech\n",
      "  - General scientific entities\n",
      "\n",
      "Model 2 (nlp_biomed) specializes in:\n",
      "  - DISEASE: Parkinson's, amnesia, depression\n",
      "  - CHEMICAL: dopamine, serotonin, glucose\n",
      "\n",
      "Model 3 (nlp_bio) specializes in:\n",
      "  - PROTEIN: sodium-potassium pump, receptors\n",
      "  - PROCESS: neurotransmission, metabolism\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Load the Three SciSpacy Models\n",
    "# This prepares us for STEP 3: SciSpacy Processing\n",
    "\n",
    "print(\"Loading SciSpacy models (this may take 1-2 minutes)...\")\n",
    "print()\n",
    "\n",
    "# MODEL 1: General scientific text processor\n",
    "# Purpose: Sentence segmentation + general scientific entities\n",
    "print(\"üì¶ Loading Model 1: en_core_sci_sm (General Scientific)\")\n",
    "print(\"   - This model understands scientific text structure\")\n",
    "print(\"   - It will break text into sentences\")\n",
    "print(\"   - It recognizes general scientific terms\")\n",
    "nlp_base = spacy.load(\"en_core_sci_sm\")\n",
    "print(\"   ‚úì Loaded\\n\")\n",
    "\n",
    "# MODEL 2: Biomedical entities (diseases and chemicals)\n",
    "# Purpose: Find diseases, symptoms, and chemical compounds\n",
    "print(\"üì¶ Loading Model 2: en_ner_bc5cdr_md (Diseases & Chemicals)\")\n",
    "print(\"   - Trained on biomedical literature\")\n",
    "print(\"   - Recognizes: diseases, symptoms, drugs, chemicals\")\n",
    "nlp_biomed = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "print(\"   ‚úì Loaded\\n\")\n",
    "\n",
    "# MODEL 3: Biological processes and proteins\n",
    "# Purpose: Find biological processes, molecular functions\n",
    "print(\"üì¶ Loading Model 3: en_ner_bionlp13cg_md (Biological Processes)\")\n",
    "print(\"   - Trained on molecular biology papers\")\n",
    "print(\"   - Recognizes: proteins, genes, cellular processes\")\n",
    "nlp_bio = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "print(\"   ‚úì Loaded\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ All 3 SciSpacy models loaded and ready!\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Show what each model can do\n",
    "print(\"MODEL CAPABILITIES:\")\n",
    "print()\n",
    "print(\"Model 1 (nlp_base) can identify:\")\n",
    "print(\"  - Sentences, tokens, parts of speech\")\n",
    "print(\"  - General scientific entities\")\n",
    "print()\n",
    "print(\"Model 2 (nlp_biomed) specializes in:\")\n",
    "print(\"  - DISEASE: Parkinson's, amnesia, depression\")\n",
    "print(\"  - CHEMICAL: dopamine, serotonin, glucose\")\n",
    "print()\n",
    "print(\"Model 3 (nlp_bio) specializes in:\")\n",
    "print(\"  - PROTEIN: sodium-potassium pump, receptors\")\n",
    "print(\"  - PROCESS: neurotransmission, metabolism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36807dd3-6802-4ce6-9b2a-84e47bda7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Extract Sentences\n",
    "# This is STEP 3a: Sentence segmentation\n",
    "\n",
    "print(\"Processing text to extract sentences...\")\n",
    "print(\"(This may take a few minutes for a 600-page book)\")\n",
    "print()\n",
    "\n",
    "# Process the cleaned text with the base model\n",
    "# This creates a spaCy \"Doc\" object that contains linguistic information\n",
    "doc_base = nlp_base(cleaned_text)\n",
    "\n",
    "# Extract all sentences into a list\n",
    "sentences = []\n",
    "for i, sent in enumerate(doc_base.sents):\n",
    "    sentence_data = {\n",
    "        \"id\": i,  # Unique sentence ID (0, 1, 2, ...)\n",
    "        \"text\": sent.text.strip(),  # The actual sentence text\n",
    "        \"start_char\": sent.start_char,  # Where it starts in the original text\n",
    "        \"end_char\": sent.end_char,  # Where it ends\n",
    "        \"token_count\": len(sent)  # Number of words/tokens\n",
    "    }\n",
    "    sentences.append(sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49a89897-91dd-40b0-9daa-f8df3a7a588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SENTENCE SEGMENTATION RESULTS\n",
      "======================================================================\n",
      "Total sentences extracted: 1,443\n",
      "Average sentence length: 30.8 tokens\n",
      "Shortest sentence: 3 tokens\n",
      "Longest sentence: 247 tokens\n",
      "\n",
      "FIRST 5 SENTENCES:\n",
      "----------------------------------------------------------------------\n",
      "[Sentence 0] (21 tokens)\n",
      "  ## CHAPTER\n",
      "\n",
      "## Introduction STARTING OUT: A Spark of Awe in the Darkness Who Are We?\n",
      "  Position: characters 0-84\n",
      "\n",
      "[Sentence 1] (116 tokens)\n",
      "  In Pursuit of Principles How We Know What We Know RESEARCH METHODS: Magnetic Resonance Imaging Thinking Critically about the Brain The Big Questions in Cognitive Neuroscience The Payoffs of Cognitive Neuroscience 1\n",
      "\n",
      "## STARTING OUT:\n",
      "\n",
      "## A Spark of Awe in the Darkness On October 9, 1604, a brilliant spark of light grew to life in the darkness of the night sky over Europe.  A few days later, the astronomer Johannes Kepler began to gaze up at the new star that had appeared in the void, outshining all its peers, visible for a time even through the brightness of the day.\n",
      "  Position: characters 84-658\n",
      "\n",
      "[Sentence 2] (39 tokens)\n",
      "  Kepler wrote extensively on the astronomical properties of the new star, or stella nova, whose sudden appearance challenged the conventional wisdom that the heavens were fixed and unchanging (Kepler, [1604] 2004).\n",
      "  Position: characters 659-872\n",
      "\n",
      "[Sentence 3] (16 tokens)\n",
      "  Over the ensuing months, the new star faded gradually back into the celestial background.\n",
      "  Position: characters 873-962\n",
      "\n",
      "[Sentence 4] (19 tokens)\n",
      "  Nothing similar has appeared in our skies to surpass it since then,  even four centuries later.\n",
      "  Position: characters 963-1058\n",
      "\n",
      "3 SENTENCES FROM MIDDLE OF BOOK (around sentence 721):\n",
      "----------------------------------------------------------------------\n",
      "[Sentence 721]\n",
      "  As it turns out, this architecture is both ancient and essential to survival.\n",
      "\n",
      "[Sentence 722]\n",
      "  In this chapter, we'll start by looking at the basic motivational circuitry needed by every animal to keep itself alive.\n",
      "\n",
      "[Sentence 723]\n",
      "  By the end of the chapter,  we will see how these same motivational mechanisms become disrupted, leading to the self-  destructive behaviors of addiction.\n",
      "\n",
      "‚úÖ Sentence segmentation complete!\n",
      "üíæ Sentences stored in variable: sentences (list of dictionaries)\n"
     ]
    }
   ],
   "source": [
    "# Display statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"SENTENCE SEGMENTATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total sentences extracted: {len(sentences):,}\")\n",
    "print(f\"Average sentence length: {sum(s['token_count'] for s in sentences) / len(sentences):.1f} tokens\")\n",
    "print(f\"Shortest sentence: {min(s['token_count'] for s in sentences)} tokens\")\n",
    "print(f\"Longest sentence: {max(s['token_count'] for s in sentences)} tokens\")\n",
    "print()\n",
    "\n",
    "# Show first 5 sentences as examples\n",
    "print(\"FIRST 5 SENTENCES:\")\n",
    "print(\"-\" * 70)\n",
    "for sent in sentences[:5]:\n",
    "    print(f\"[Sentence {sent['id']}] ({sent['token_count']} tokens)\")\n",
    "    print(f\"  {sent['text']}\")  \n",
    "    print(f\"  Position: characters {sent['start_char']}-{sent['end_char']}\")\n",
    "    print()\n",
    "\n",
    "# Show a few sentences from the middle of the book\n",
    "middle_index = len(sentences) // 2\n",
    "print(f\"3 SENTENCES FROM MIDDLE OF BOOK (around sentence {middle_index}):\")\n",
    "print(\"-\" * 70)\n",
    "for sent in sentences[middle_index:middle_index+3]:\n",
    "    print(f\"[Sentence {sent['id']}]\")\n",
    "    print(f\"  {sent['text']}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Sentence segmentation complete!\")\n",
    "print(\"üíæ Sentences stored in variable: sentences (list of dictionaries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05c22e6e-5acf-4dc3-9be5-0356b2b5c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities with Model 1 (General Scientific)...\n",
      "\n",
      "======================================================================\n",
      "MODEL 1 (GENERAL SCIENTIFIC) RESULTS\n",
      "======================================================================\n",
      "Total entities found: 9,869\n",
      "\n",
      "Entity types found: 1\n",
      "\n",
      "TOP ENTITY TYPES:\n",
      "----------------------------------------------------------------------\n",
      "ENTITY                9869 mentions (4603 unique terms)\n",
      "\n",
      "EXAMPLES FROM TOP 3 ENTITY TYPES:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ENTITY:\n",
      "  ‚Ä¢ imprinted\n",
      "  ‚Ä¢ emergence\n",
      "  ‚Ä¢ Halaas\n",
      "  ‚Ä¢ bunker\n",
      "  ‚Ä¢ Seymour\n",
      "\n",
      "‚úÖ Model 1 entity extraction complete!\n",
      "üíæ Entities stored in variable: entities_model1\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Extract Entities with Model 1 (General Scientific)\n",
    "# This is STEP 3b: Entity recognition (Part 1)\n",
    "\n",
    "print(\"Extracting entities with Model 1 (General Scientific)...\")\n",
    "print()\n",
    "\n",
    "# The doc_base we created earlier already contains entity information\n",
    "# Now we'll extract it into a structured format\n",
    "\n",
    "entities_model1 = []\n",
    "\n",
    "for ent in doc_base.ents:\n",
    "    entity_data = {\n",
    "        \"text\": ent.text,  # The actual text of the entity\n",
    "        \"label\": ent.label_,  # What type of entity it is\n",
    "        \"start_char\": ent.start_char,  # Position in original text\n",
    "        \"end_char\": ent.end_char,\n",
    "        \"model\": \"general_sci\"  # Which model found it\n",
    "    }\n",
    "    entities_model1.append(entity_data)\n",
    "\n",
    "# Analyze what we found\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 1 (GENERAL SCIENTIFIC) RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total entities found: {len(entities_model1):,}\")\n",
    "print()\n",
    "\n",
    "# Group entities by type to see what categories we found\n",
    "entity_types = defaultdict(list)\n",
    "for ent in entities_model1:\n",
    "    entity_types[ent['label']].append(ent['text'])\n",
    "\n",
    "print(f\"Entity types found: {len(entity_types)}\")\n",
    "print()\n",
    "\n",
    "# Show top entity types by frequency\n",
    "print(\"TOP ENTITY TYPES:\")\n",
    "print(\"-\" * 70)\n",
    "sorted_types = sorted(entity_types.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "for label, texts in sorted_types[:10]:  # Show top 10 types\n",
    "    unique_terms = len(set(texts))\n",
    "    print(f\"{label:20} {len(texts):5} mentions ({unique_terms} unique terms)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show examples for the most common entity types\n",
    "print(\"EXAMPLES FROM TOP 3 ENTITY TYPES:\")\n",
    "print(\"-\" * 70)\n",
    "for label, texts in sorted_types[:3]:\n",
    "    print(f\"\\n{label}:\")\n",
    "    # Show first 5 unique examples\n",
    "    unique_examples = list(set(texts))[:5]\n",
    "    for example in unique_examples:\n",
    "        print(f\"  ‚Ä¢ {example}\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Model 1 entity extraction complete!\")\n",
    "print(\"üíæ Entities stored in variable: entities_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cad7e7a0-abae-4570-a52c-d462c25e30fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities with Model 2 (Biomedical - Diseases & Chemicals)...\n",
      "(This processes the text again with a specialized model)\n",
      "\n",
      "======================================================================\n",
      "MODEL 2 (BIOMEDICAL) RESULTS\n",
      "======================================================================\n",
      "Total entities found: 429\n",
      "\n",
      "ENTITY TYPES FOUND:\n",
      "----------------------------------------------------------------------\n",
      "DISEASE                215 mentions (138 unique terms)\n",
      "CHEMICAL               214 mentions (61 unique terms)\n",
      "\n",
      "EXAMPLES OF DISEASES FOUND:\n",
      "----------------------------------------------------------------------\n",
      "  ‚Ä¢ deficit in memory recollection\n",
      "  ‚Ä¢ movement disorders\n",
      "  ‚Ä¢ peacefulness\n",
      "  ‚Ä¢ major depression\n",
      "  ‚Ä¢ Schizophrenia\n",
      "  ‚Ä¢ compulsive shopping\n",
      "  ‚Ä¢ nausea and vomiting\n",
      "  ‚Ä¢ delirium\n",
      "  ‚Ä¢ malnutrition\n",
      "  ‚Ä¢ teasing\n",
      "\n",
      "EXAMPLES OF CHEMICALS FOUND:\n",
      "----------------------------------------------------------------------\n",
      "  ‚Ä¢ smoking\n",
      "  ‚Ä¢ Higgins\n",
      "  ‚Ä¢ smoke\n",
      "  ‚Ä¢ metallurgy\n",
      "  ‚Ä¢ take-\n",
      "  ‚Ä¢ buprenorphine\n",
      "  ‚Ä¢ varenicline\n",
      "  ‚Ä¢ psychotomimetic\n",
      "  ‚Ä¢ bupropion\n",
      "  ‚Ä¢ heroin\n",
      "\n",
      "‚úÖ Model 2 entity extraction complete!\n",
      "üíæ Entities stored in variable: entities_model2\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: Extract Entities with Model 2 (Diseases & Chemicals)\n",
    "# This is STEP 3b: Entity recognition (Part 2)\n",
    "\n",
    "print(\"Extracting entities with Model 2 (Biomedical - Diseases & Chemicals)...\")\n",
    "print(\"(This processes the text again with a specialized model)\")\n",
    "print()\n",
    "\n",
    "# Process the same cleaned text with the biomedical model\n",
    "doc_biomed = nlp_biomed(cleaned_text)\n",
    "\n",
    "# Extract entities from this model\n",
    "entities_model2 = []\n",
    "\n",
    "for ent in doc_biomed.ents:\n",
    "    entity_data = {\n",
    "        \"text\": ent.text,\n",
    "        \"label\": ent.label_,\n",
    "        \"start_char\": ent.start_char,\n",
    "        \"end_char\": ent.end_char,\n",
    "        \"model\": \"biomedical\"\n",
    "    }\n",
    "    entities_model2.append(entity_data)\n",
    "\n",
    "# Analyze results\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 2 (BIOMEDICAL) RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total entities found: {len(entities_model2):,}\")\n",
    "print()\n",
    "\n",
    "# Group by type\n",
    "entity_types_biomed = defaultdict(list)\n",
    "for ent in entities_model2:\n",
    "    entity_types_biomed[ent['label']].append(ent['text'])\n",
    "\n",
    "print(\"ENTITY TYPES FOUND:\")\n",
    "print(\"-\" * 70)\n",
    "for label, texts in entity_types_biomed.items():\n",
    "    unique_terms = len(set(texts))\n",
    "    print(f\"{label:20} {len(texts):5} mentions ({unique_terms} unique terms)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show specific examples\n",
    "print(\"EXAMPLES OF DISEASES FOUND:\")\n",
    "print(\"-\" * 70)\n",
    "if 'DISEASE' in entity_types_biomed:\n",
    "    diseases = list(set(entity_types_biomed['DISEASE']))[:10]\n",
    "    for disease in diseases:\n",
    "        print(f\"  ‚Ä¢ {disease}\")\n",
    "else:\n",
    "    print(\"  (No diseases found with this label)\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"EXAMPLES OF CHEMICALS FOUND:\")\n",
    "print(\"-\" * 70)\n",
    "if 'CHEMICAL' in entity_types_biomed:\n",
    "    chemicals = list(set(entity_types_biomed['CHEMICAL']))[:10]\n",
    "    for chemical in chemicals:\n",
    "        print(f\"  ‚Ä¢ {chemical}\")\n",
    "else:\n",
    "    print(\"  (No chemicals found with this label)\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Model 2 entity extraction complete!\")\n",
    "print(\"üíæ Entities stored in variable: entities_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "995d551b-abad-4937-aae4-85eb95f8a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities with Model 3 (Biological Processes & Proteins)...\n",
      "\n",
      "======================================================================\n",
      "MODEL 3 (BIOLOGICAL PROCESSES) RESULTS\n",
      "======================================================================\n",
      "Total entities found: 1,608\n",
      "\n",
      "ENTITY TYPES FOUND:\n",
      "----------------------------------------------------------------------\n",
      "ORGAN                       392 mentions (37 unique)\n",
      "ORGANISM                    269 mentions (69 unique)\n",
      "SIMPLE_CHEMICAL             209 mentions (74 unique)\n",
      "CELL                        123 mentions (28 unique)\n",
      "CANCER                      119 mentions (54 unique)\n",
      "GENE_OR_GENE_PRODUCT        118 mentions (74 unique)\n",
      "TISSUE                       90 mentions (32 unique)\n",
      "PATHOLOGICAL_FORMATION       69 mentions (37 unique)\n",
      "MULTI_TISSUE_STRUCTURE       54 mentions (34 unique)\n",
      "CELLULAR_COMPONENT           53 mentions (29 unique)\n",
      "ORGANISM_SUBSTANCE           49 mentions (8 unique)\n",
      "ORGANISM_SUBDIVISION         30 mentions (11 unique)\n",
      "ANATOMICAL_SYSTEM            28 mentions (13 unique)\n",
      "IMMATERIAL_ANATOMICAL_ENTITY     3 mentions (2 unique)\n",
      "DEVELOPING_ANATOMICAL_STRUCTURE     1 mentions (1 unique)\n",
      "AMINO_ACID                    1 mentions (1 unique)\n",
      "\n",
      "SAMPLE ENTITIES BY TYPE:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ORGAN:\n",
      "  ‚Ä¢ skin\n",
      "  ‚Ä¢ lungs\n",
      "  ‚Ä¢ brain  \n",
      "  ‚Ä¢ dorsal striatum\n",
      "  ‚Ä¢ roots\n",
      "\n",
      "DEVELOPING_ANATOMICAL_STRUCTURE:\n",
      "  ‚Ä¢ stella\n",
      "\n",
      "SIMPLE_CHEMICAL:\n",
      "  ‚Ä¢ water\n",
      "  ‚Ä¢ gum\n",
      "  ‚Ä¢ meth\n",
      "  ‚Ä¢ dialysate\n",
      "  ‚Ä¢ oxycodone\n",
      "\n",
      "CANCER:\n",
      "  ‚Ä¢ brainstem\n",
      "  ‚Ä¢ prefrontal cortex\n",
      "  ‚Ä¢ brain hemisphere\n",
      "  ‚Ä¢ Nicola\n",
      "  ‚Ä¢ Connectional\n",
      "\n",
      "ORGANISM:\n",
      "  ‚Ä¢ J.\n",
      "  ‚Ä¢ Jequier\n",
      "  ‚Ä¢ Y.\n",
      "  ‚Ä¢ patch\n",
      "  ‚Ä¢ dopamine neuron\n",
      "\n",
      "‚úÖ Model 3 entity extraction complete!\n",
      "üíæ Entities stored in variable: entities_model3\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: Extract Entities with Model 3 (Biological Processes)\n",
    "# This is STEP 3b: Entity recognition (Part 3)\n",
    "\n",
    "print(\"Extracting entities with Model 3 (Biological Processes & Proteins)...\")\n",
    "print()\n",
    "\n",
    "# Process with the biological model\n",
    "doc_bio = nlp_bio(cleaned_text)\n",
    "\n",
    "# Extract entities\n",
    "entities_model3 = []\n",
    "\n",
    "for ent in doc_bio.ents:\n",
    "    entity_data = {\n",
    "        \"text\": ent.text,\n",
    "        \"label\": ent.label_,\n",
    "        \"start_char\": ent.start_char,\n",
    "        \"end_char\": ent.end_char,\n",
    "        \"model\": \"biological\"\n",
    "    }\n",
    "    entities_model3.append(entity_data)\n",
    "\n",
    "# Analyze results\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 3 (BIOLOGICAL PROCESSES) RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total entities found: {len(entities_model3):,}\")\n",
    "print()\n",
    "\n",
    "# Group by type\n",
    "entity_types_bio = defaultdict(list)\n",
    "for ent in entities_model3:\n",
    "    entity_types_bio[ent['label']].append(ent['text'])\n",
    "\n",
    "print(\"ENTITY TYPES FOUND:\")\n",
    "print(\"-\" * 70)\n",
    "for label, texts in sorted(entity_types_bio.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    unique_terms = len(set(texts))\n",
    "    print(f\"{label:25} {len(texts):5} mentions ({unique_terms} unique)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show examples of different biological entity types\n",
    "print(\"SAMPLE ENTITIES BY TYPE:\")\n",
    "print(\"-\" * 70)\n",
    "for label, texts in list(entity_types_bio.items())[:5]:  # Show first 5 types\n",
    "    print(f\"\\n{label}:\")\n",
    "    unique_examples = list(set(texts))[:5]\n",
    "    for example in unique_examples:\n",
    "        print(f\"  ‚Ä¢ {example}\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Model 3 entity extraction complete!\")\n",
    "print(\"üíæ Entities stored in variable: entities_model3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b76fba-be86-4358-8066-363c42dad4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raglab",
   "language": "python",
   "name": "raglab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
