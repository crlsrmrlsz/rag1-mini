# Optional local LLM integration using llama.cpp
